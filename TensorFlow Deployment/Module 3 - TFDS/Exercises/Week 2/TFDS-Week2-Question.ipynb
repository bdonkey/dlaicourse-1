{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 2-Question.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "qRLGSMDzM-dl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c05P9g5WjizZ"
      },
      "source": [
        "# Classify structured data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxyBFc_kKazA"
      },
      "source": [
        "## Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LuOWVJBz8a6G",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9dEreb4QKizj",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KCEhSZcULZ9n"
      },
      "source": [
        "## Use Pandas to create a dataframe\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) is a Python library with many helpful utilities for loading and working with structured data. We will use Pandas to download the dataset from a URL, and load it into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "REZ57BXCLdfG",
        "colab": {}
      },
      "source": [
        "URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
        "dataframe = pd.read_csv(URL)\n",
        "dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u0zhLtQqMPem"
      },
      "source": [
        "## Split the dataframe into train, validation, and test\n",
        "\n",
        "The dataset we downloaded was a single CSV file. We will split this into train, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YEOpw7LhMYsI",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84ef46LXMfvu"
      },
      "source": [
        "## Create an input pipeline using tf.data\n",
        "\n",
        "Next, we will wrap the dataframes with [tf.data](https://www.tensorflow.org/guide/datasets). This will enable us  to use feature columns as a bridge to map from the columns in the Pandas dataframe to features used to train the model. If we were working with a very large CSV file (so large that it does not fit into memory), we would use tf.data to read it from disk directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NkcaMYP-MsRe",
        "colab": {}
      },
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  # Use Pandas dataframe's pop method to get list of targets\n",
        "  labels = # YOUR CODE HERE\n",
        "  # Create a tf.data.Dataset from the dataframe and labels.\n",
        "  ds = # YOUR CODE HERE\n",
        "  if shuffle:\n",
        "    # Shuffle dataset\n",
        "    ds = # YOUR CODE HERE\n",
        "  # Batch dataset with specified batch_size parameter\n",
        "  ds = # YOUR CODE HERE\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CXbbXkJvMy34",
        "colab": {}
      },
      "source": [
        "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qRLGSMDzM-dl"
      },
      "source": [
        "## Understand the input pipeline\n",
        "\n",
        "Now that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CSBo3dUVNFc9",
        "colab": {}
      },
      "source": [
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  print('Every feature:', list(feature_batch.keys()))\n",
        "  print('A batch of ages:', feature_batch['age'])\n",
        "  print('A batch of targets:', label_batch )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OT5N6Se-NQsC"
      },
      "source": [
        "We can see that the dataset returns a dictionary of column names (from the dataframe) that map to column values from rows in the dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ttIvgLRaNoOQ"
      },
      "source": [
        "## Demonstrate several types of feature column\n",
        "TensorFlow provides many types of feature columns. In this section, we will create several types of feature columns, and demonstrate how they transform a column from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mxwiHFHuNhmf",
        "colab": {}
      },
      "source": [
        "# Try to demonstrate several types of feature columns by getting an example\n",
        "example_batch = next(iter(train_ds))[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0wfLB8Q3N3UH",
        "colab": {}
      },
      "source": [
        "# A utility method to create a feature column and to transform a batch of data\n",
        "def demo(feature_column):\n",
        "  feature_layer = layers.DenseFeatures(feature_column)\n",
        "  print(feature_layer(example_batch).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q7OEKe82N-Qb"
      },
      "source": [
        "### Numeric columns\n",
        "The output of a feature column becomes the input to the model (using the demo function defined above, we will be able to see exactly how each column from the dataframe is transformed). A [numeric column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column) is the simplest type of column. It is used to represent real valued features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZTZ0HnHOCxC",
        "colab": {}
      },
      "source": [
        "# Create a numeric feature column out of 'age' and demo it.\n",
        "age = # YOUR CODE HERE\n",
        "demo(age)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7a6ddSyzOKpq"
      },
      "source": [
        "In the heart disease dataset, most columns from the dataframe are numeric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IcSxUoYgOlA1"
      },
      "source": [
        "### Bucketized columns\n",
        "Often, you don't want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. Consider raw data that represents a person's age. Instead of representing age as a numeric column, we could split the age into several buckets using a [bucketized column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wJ4Wt3SAOpTQ",
        "colab": {}
      },
      "source": [
        "# Create a bucketized feature column out of 'age' with the following boundaries and demo it.\n",
        "boundaries = [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n",
        "age_buckets = # YOUR CODE HERE \n",
        "demo(age_buckets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-me1NKJ4BIEB",
        "colab_type": "text"
      },
      "source": [
        "Notice the one-hot values above describe which age range each row matches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r1tArzewPb-b"
      },
      "source": [
        "### Categorical columns\n",
        "In this dataset, thal is represented as a string (e.g. 'fixed', 'normal', or 'reversible'). We cannot feed strings directly to a model. Instead, we must first map them to numeric values. The categorical vocabulary columns provide a way to represent strings as a one-hot vector (much like you have seen above with age buckets). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DJ6QnSHkPtOC",
        "colab": {}
      },
      "source": [
        "# Create a categorical vocabulary column out of the above mentioned categories with the key specified as 'thal'\n",
        "thal = # YOUR CODE HERE\n",
        "\n",
        "# Create an indicator column out of the created categorical column\n",
        "thal_one_hot = # YOUR CODE HERE\n",
        "demo(thal_one_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQT4zecNBtji",
        "colab_type": "text"
      },
      "source": [
        "The vocabulary can be passed as a list using [categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list), or loaded from a file using [categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LEFPjUr6QmwS"
      },
      "source": [
        "### Embedding columns\n",
        "Suppose instead of having just a few possible strings, we have thousands (or more) values per category. For a number of reasons, as the number of categories grow large, it becomes infeasible to train a neural network using one-hot encodings. We can use an embedding column to overcome this limitation. Instead of representing the data as a one-hot vector of many dimensions, an [embedding column](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column) represents that data as a lower-dimensional, dense vector in which each cell can contain any number, not just 0 or 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hSlohmr2Q_UU",
        "colab": {}
      },
      "source": [
        "# Create an embedding column out of the categorical vocabulary you just created (thal)\n",
        "\n",
        "thal_embedding = # YOUR CODE HERE\n",
        "# You can tune the size of the embedding with the dimension parameter.\n",
        "\n",
        "demo(thal_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "urFCAvTVRMpB"
      },
      "source": [
        "### Hashed feature columns\n",
        "\n",
        "Another way to represent a categorical column with a large number of values is to use a [categorical_column_with_hash_bucket](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket). This feature column calculates a hash value of the input, then selects one of the `hash_bucket_size` buckets to encode a string. When using this column, you do not need to provide the vocabulary, and you can choose to make the number of hash_buckets significantly smaller than the number of actual categories to save space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YHU_Aj2nRRDC",
        "colab": {}
      },
      "source": [
        "# Create a hashed feature column with 'thal' as the key\n",
        "thal_hashed = # YOUR CODE HERE\n",
        "demo(feature_column.indicator_column(thal_hashed))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fB94M27DRXtZ"
      },
      "source": [
        "### Crossed feature columns\n",
        "Combining features into a single feature, better known as [feature crosses](https://developers.google.com/machine-learning/glossary/#feature_cross), enables a model to learn separate weights for each combination of features. Here, we will create a new feature that is the cross of age and thal. Note that `crossed_column` does not build the full table of all possible combinations (which could be very large). Instead, it is backed by a `hashed_column`, so you can choose how large the table is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oaPVERd9Rep6",
        "colab": {}
      },
      "source": [
        "# Create a crossed column using the bucketized column (age_buckets) and categorical vocabulary column (thal) previously created \n",
        "crossed_feature = # YOUR CODE HERE\n",
        "demo(feature_column.indicator_column(crossed_feature))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ypkI9zx6Rj1q"
      },
      "source": [
        "## Choose which columns to use\n",
        "We have seen how to use several types of feature columns. Now we will use them to train a model. The goal of this exercise is to show you the complete code needed to work with feature columns. We have selected a few columns to train our model below arbitrarily.\n",
        "\n",
        "If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu8bJWmCScfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pV4tSI3SkuX",
        "colab_type": "text"
      },
      "source": [
        "You can use the above list of column datatypes to map the appropriate feature column to every column in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4PlLY7fORuzA",
        "colab": {}
      },
      "source": [
        "feature_columns = []\n",
        "\n",
        "# numeric cols\n",
        "# Insert the list of columns that have a numeric datatype down here\n",
        "numeric_coumns = # YOUR CODE HERE\n",
        "for header in numeric_coumns:\n",
        "  # Create a numeric feature column  out of the header here\n",
        "  numeric_feature_column = # YOUR CODE HERE\n",
        "  feature_columns.append(numeric_feature_column)\n",
        "\n",
        "# bucketized cols\n",
        "# Create a bucketized feature column out of the numeric column that you've already created (age)\n",
        "age_buckets = # YOUR CODE HERE\n",
        "feature_columns.append(age_buckets)\n",
        "\n",
        "# indicator cols\n",
        "# Create a categorical vocabulary column out of the categories ('fixed', 'normal', 'reversible') with the key specified as 'thal'\n",
        "thal = # YOUR CODE HERE\n",
        "# Create an indicator column out of the created categorical column to one-hot encode it\n",
        "thal_one_hot = # YOUR CODE HERE\n",
        "feature_columns.append(thal_one_hot)\n",
        "\n",
        "# embedding cols\n",
        "# Create an embedding column out of the categorical vocabulary you just created (thal)\n",
        "thal_embedding = # YOUR CODE HERE\n",
        "feature_columns.append(thal_embedding)\n",
        "\n",
        "# crossed cols\n",
        "# Create a crossed column using the bucketized column (age_buckets) and categorical vocabulary column (thal) previously created \n",
        "crossed_feature = # YOUR CODE HERE\n",
        "# Create an indicator column out of the crossed column to one-hot encode it\n",
        "crossed_feature = # YOUR CODE HERE\n",
        "feature_columns.append(crossed_feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-nDp8krS_ts"
      },
      "source": [
        "### Create a feature layer\n",
        "Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6o-El1R2TGQP",
        "colab": {}
      },
      "source": [
        "# Create a DenseFeature layer to pass the feature column you just created\n",
        "feature_layer = # YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8cf6vKfgTH0U"
      },
      "source": [
        "Earlier, we used a small batch size to demonstrate how feature columns worked. We create a new input pipeline with a larger batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gcemszoGSse_",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bBx4Xu0eTXWq"
      },
      "source": [
        "## Create, compile, and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YJPPb3xTPeZ",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnFmMOW0Tcaa",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
